American Sign Language (ASL) recognition system

1. Create a Virtual Environment (Optional)

   ```bash
   python -m venv myenv
   ```

2. Activate the Virtual Environment

   - On Windows:

     ```bash
     myenv\Scripts\activate
     ```

   - On macOS/Linux:

     ```bash
     source myenv/bin/activate
     ```

3. Install Required Packages

   ```bash
   pip install cv2 mediapipe numpy tensorflow
   ```

 3.2. Running the Application

 Collecting Datasets

1. Run Data Collection Script

   ```bash
   python index.py --collect
   ```

2. Collect Data for Gestures

   - When prompted, enter the gesture action to collect (e.g., 'A', 'V', '1', '3') or type 'exit' to stop.
   - A window will open displaying the webcam feed.
   - Perform the gesture in front of the camera.
   - Press 'q' to stop early if needed.
   - The collected data will be saved in the `gesture_data` directory.

 Training the Model

1. Run Training Script

   ```bash
   python index.py --train
   ```

2. Training Process

   - The model will train for 100 epochs (you can adjust this in the code).
   - Training progress and accuracy will be displayed in the console.
   - After training, the model is saved as `action.h5`.

 Running Real-Time Gesture Recognition

1. Run Recognition Script

   ```bash
   python index.py --recognize
   ```

2. Perform Gestures

   - A window will open displaying the webcam feed.
   - Perform the trained gestures ('A', 'V', '1', '3') in front of the camera.
   - The recognized gesture will be displayed on the screen.
   - Press 'q' to exit the application.
